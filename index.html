
<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Neural Matching Fields: Implicit Representation of Matching Cost for Semantic Correspondence</title>
	<meta property="og:image" content="https://KU-CVLAB.github.io/NeMF"/>
	<meta property="og:title" content=Neural Matching Fields: Implicit Representation of Matching Cost for Semantic Correspondence" />
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Neural Matching Fields: Implicit Representation of Matching Cost for Semantic Correspondence</span>
		<table align=center width=1100px>
			<table align=center width=1200px>
				<tr>
					<td align=center width=180px>
						<center>
							<span style="font-size:22px"><a href="https://sunghwanhong.github.io/">Sunghwan Hong<sup>1</sup></a></span>
						</center>
					</td>
					<td align=center width=180px>
						<center>
							<span style="font-size:22px"><a href="https://scholar.google.com/citations?view_op=list_works&hl=ko&user=pbWpW-MAAAAJ">Jisu Nam<sup>1</sup></a></span>
						</center>
					</td>
					<td align=center width=180px>
						<center>
							<span style="font-size:22px"><a href="https://seokju-cho.github.io/">Seokju Cho<sup>1</sup></a></span>
						</center>
					</td>
					<td align=center width=180px>
						<center>
							<span style="font-size:22px"><a href="https://pndong.github.io/">Susung Hong<sup>1</sup></a></span>
						</center>
					</td>
					<td align=center width=180px>
						<center>
							<span style="font-size:22px"><a href="https://scholar.google.com/citations?hl=ko&user=3s4zIroAAAAJ">Sangryul Jeon<sup>2</sup></a></span>
						</center>
					</td>
					<td align=center width=180px>
						<center>
							<span style="font-size:22px"><a href="https://scholar.google.com/citations?hl=ko&user=3REUPXYAAAAJ">Dongbo Min<sup>3</sup></a></span>
						</center>
					</td>
					<td align=center width=180px>
						<center>
							<span style="font-size:22px"><a href="https://cvlab.korea.ac.kr/members">Seungryong Kim<sup>1</sup></a></span>
						</center>
					</td>
				</tr>
			</table>
						
			<table align=center width=800px>
				<tr>
					<td align=center width=150px>
						<center>
							<span style="font-size:20px">Korea University<sup>1</sup></a></span>
						</center>
					</td>
			
					<td align=center width=150px>
						<center>
							<span style="font-size:20px">UC Berkeley<sup>2</sup></a></span>
						</center>
					</td>
					
					<td align=center width=150px>
						<center>
							<span style="font-size:20px">Ewha Womans University<sup>3</sup></a></span>
						</center>
					</td>

				</tr>
			</table>
			
			<hr>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:20px">[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:20px"><a href="https://github.com/KU-CVLAB/NeMF/">[Code]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>
	<hr>
	<br>
	
	<center>
		<table align=center width=850px>
			<tr>
				<td width=420px>
					<center>
						<img class="round" style="width:900px" src="./resources/figure_1.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					Figure 1: Unlike existing methods that explicitly compute and store discrete matching field defined at low resolution, we implicitly represent a high-dimensional 4D matching field with deep fully-connected networks defined at arbitrary original image resolution.
				</td>
			</tr>
		</table>
	</center>
	<br>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Existing pipelines of semantic correspondence commonly include extracting high level semantic features for the invariance against intra-class variations and background clutters. This architecture, however, inevitably results in a low-resolution matching field that additionally requires an ad-hoc interpolation process as a post-processing for converting it into a high-resolution one, certainly limiting the 
														     
														     performance of matching results. To overcome this, inspired by recent success of implicit neural representation, we present a novel method for semantic correspondence, called neural matching field (NeMF). However, complicacy and high-dimensionality of a 4D matching field are the major hindrances. To address them, we propose a cost embedding network consisting of convolution and self-attention layers to process the coarse cost volume to obtain cost feature representation, which is used as a guidance for establishing high-precision matching field through the following fully-connected network. Although this may help to better structure the matching field, learning a high-dimensional matching field remains challenging mainly due to computational complexity, since a na√Øve ex- haustive inference would require querying from all pixels in the 4D space to infer pixel-wise correspondences. To overcome this, in the training phase, we randomly sample matching candidates. In the inference phase, we propose a novel inference approach which iteratively performs PatchMatch-based inference and coordinate optimization at test time. With the proposed method, competitive results are at- tained on several standard benchmarks for semantic correspondence.
			</td>
		</tr>
	</table>
	<br>
	<hr>

	<center><h1> Overview Architecture </h1></center>
	
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:900px" src="./resources/figure_2.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Figure 2: Given a pair of images as an input, we first extract features using CNNs and compute an initial noisy cost volume at low resolution. We feed the noisy cost volume with the proposed encoder consisting of convolution and Transformer, and decode with deep fully connected networks by taking the encoded cost and coordinates as inputs.
				</td>
			</tr>
		</center>
	</table>
	<br>
	<hr>
														 
	<center><h1> Training </h1></center>
	
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:900px" src="./resources/figure_3.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Figure 3: Overview of neural matching field optimization: Given an encoded cost, we randomly
					sample coordinates from uniform distribution. The random coordinates and ground-truth coordinate
					are then processed altogether to obtain the matching scores and the cross-entropy loss is computed
					for the training signal.
			</td>
			</tr>
		</center>
	</table>
	<br>
	<hr>
														 
	<center><h1> Inference </h1></center>
	
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:900px" src="./resources/figure_4.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Figure 4: Illustration of the proposed PatchMatch and coordinate optimization: With the learned
					neural matching field, the proposed PatchMatch injects explicit smoothness and reduces the search
					range. The subsequent optimization strategy searches for a location that maximizes the score of MLP.
			</td>
			</tr>
		</center>
	</table>
	<br>
	<hr>
	
        <center><h1> Quantitative Results </h1></center>

	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/quan_on_std.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Table 1: Quantitative evaluation on standard benchmarks : Higher PCK is better. The best results are in bold, and the second best results are underlined. All results are taken from the papers. Eval. Reso.: Evaluation Resolution, Flow Reso.: Flow Resolution.</td>
			</tr>
		</center>
	</table>
	<br>
        <table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/quan_spair.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Table 2: Per-class quantitative evaluation on SPair-71k benchmark.</td>
			</tr>
		</center>
	</table>
	<br>
	<hr>


	
	<center><h1> Qualitative Results </h1></center>

	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/qual_on_pf_pascal.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<hr>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Figure 5: Qualitative results on PF-PASCAL : keypoint transfer results by (a), (c) CATs and (b), (d) NeMF. Green and red line denote correct and wrong prediction (Œ±img = 0.1), respectively. Note that correspondences are estimated at the original resolutions of images.
				</td>
			</tr>
		</center>
	</table>
	<br>
	<hr>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

